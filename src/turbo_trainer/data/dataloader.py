# -*- coding: utf-8 -*-
"""
Dataset and DataLoader for Z-Image training.

Standalone implementation - no musubi-tuner dependency.
"""

import os
import glob
import logging
import math
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import torch
from torch.utils.data import Dataset, DataLoader
from safetensors.torch import load_file

try:
    import toml
except ImportError:
    try:
        import tomli as toml
    except ImportError:
        toml = None

logger = logging.getLogger(__name__)


class ZImageLatentDataset(Dataset):
    """
    Dataset for loading pre-cached latents and text embeddings.
    Supports multiple datasets and per-dataset resolution filtering.
    """
    
    LATENT_ARCH = "zi"
    TE_SUFFIX = "_zi_te.safetensors"
    
    def __init__(
        self,
        datasets: List[Dict],
        shuffle: bool = True,
        max_sequence_length: int = 512,
    ):
        super().__init__()
        
        self.datasets = datasets
        self.shuffle = shuffle
        self.max_sequence_length = max_sequence_length
        
        self.cache_files = []
        self.resolutions = []
        
        for ds_config in datasets:
            cache_dir = Path(ds_config['cache_directory'])
            repeats = ds_config.get('num_repeats', 1)
            resolution_limit = ds_config.get('resolution_limit', None)
            
            logger.info(f"Loading dataset from: {cache_dir} (repeats={repeats}, limit={resolution_limit})")
            
            files, res_list = self._load_dataset(cache_dir, resolution_limit)
            
            # Apply repeats
            if repeats > 1:
                files = files * repeats
                res_list = res_list * repeats
            
            self.cache_files.extend(files)
            self.resolutions.extend(res_list)
            
        if len(self.cache_files) == 0:
            raise ValueError("No valid cache files found in any dataset")
            
        logger.info(f"Total samples: {len(self.cache_files)} (max_seq_len={max_sequence_length})")
    
    def _load_dataset(self, cache_dir: Path, resolution_limit: Optional[int]) -> Tuple[List[Tuple[Path, Path]], List[Tuple[int, int]]]:
        """Load files from a single directory and filter by resolution"""
        files = []
        resolutions = []
        
        # Find all latent files
        pattern = f"*_{self.LATENT_ARCH}.safetensors"
        latent_files = list(cache_dir.glob(pattern))
        
        for latent_path in latent_files:
            # Parse resolution
            res = self._parse_resolution(latent_path.stem)
            
            # Filter by resolution limit
            if resolution_limit:
                h, w = res
                if max(h, w) > resolution_limit:
                    continue
            
            # Find text encoder cache
            te_path = self._find_te_path(latent_path, cache_dir)
            
            if te_path and te_path.exists():
                files.append((latent_path, te_path))
                resolutions.append(res)
            
        return files, resolutions

    def _parse_resolution(self, name: str) -> Tuple[int, int]:
        """Parse resolution from filename (e.g., image_1024x1024_zi)"""
        parts = name.split('_')
        res = (1024, 1024) # Default
        for part in parts:
            if 'x' in part and part.replace('x', '').isdigit():
                try:
                    w, h = map(int, part.split('x'))
                    res = (h, w) # (H, W)
                    break
                except:
                    pass
        return res

    def _find_te_path(self, latent_path: Path, cache_dir: Path) -> Optional[Path]:
        """Construct text encoder cache path"""
        name = latent_path.stem
        parts = name.rsplit('_', 2)
        if len(parts) >= 3:
            base_name = parts[0]
        else:
            base_name = name.rsplit('_', 1)[0]
        
        return cache_dir / f"{base_name}{self.TE_SUFFIX}"
    
    def __len__(self) -> int:
        return len(self.cache_files)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        latent_path, te_path = self.cache_files[idx]
        
        # Load latent
        latent_data = load_file(str(latent_path))
        latent_key = next((k for k in latent_data.keys() if k.startswith('latents_')), None)
        if latent_key is None:
            raise ValueError(f"No latent key found in {latent_path}")
        latents = latent_data[latent_key]
        
        # ç¡®ä¿latentå°ºå¯¸èƒ½è¢«patch_size=2æ•´é™¤ï¼ˆä¸ºTransformerå‡†å¤‡ï¼‰
        C, H, W = latents.shape
        patch_size = 2
        
        # è®¡ç®—éœ€è¦å¡«å……çš„å°ºå¯¸
        H_padded = ((H + patch_size - 1) // patch_size) * patch_size
        W_padded = ((W + patch_size - 1) // patch_size) * patch_size
        
        if H != H_padded or W != W_padded:
            # å¡«å……latentåˆ°åˆé€‚çš„å°ºå¯¸ (left, right, top, bottom)
            latents = torch.nn.functional.pad(
                latents, 
                (0, W_padded - W, 0, H_padded - H),  # (left, right, top, bottom)
                mode='reflect'
            )
        
        # Load text encoder output
        te_data = load_file(str(te_path))
        vl_embed_key = next((k for k in te_data.keys() if 'vl_embed' in k), None)
        if vl_embed_key is None:
            raise ValueError(f"No vl_embed key found in {te_path}")
        vl_embed = te_data[vl_embed_key]
        
        # æˆªæ–­/å¡«å……åˆ° max_sequence_length
        seq_len = vl_embed.shape[0]
        if seq_len > self.max_sequence_length:
            vl_embed = vl_embed[:self.max_sequence_length]
        elif seq_len < self.max_sequence_length:
            pad_len = self.max_sequence_length - seq_len
            vl_embed = torch.nn.functional.pad(vl_embed, (0, 0, 0, pad_len), mode='constant', value=0)
        
        return {
            'latents': latents,
            'vl_embed': vl_embed,
        }


class BucketBatchSampler(torch.utils.data.Sampler):
    """
    æ”¯æŒåˆ†æ¡¶çš„ Batch Samplerã€‚
    å°†å…·æœ‰ç›¸åŒåˆ†è¾¨ç‡çš„æ ·æœ¬ç»„åˆåœ¨ä¸€èµ·ã€‚
    """
    def __init__(self, dataset, batch_size, drop_last=False, shuffle=True):
        self.dataset = dataset
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.shuffle = shuffle
        
        # æŒ‰åˆ†è¾¨ç‡åˆ†ç»„ç´¢å¼•
        self.buckets = {} # (h, w) -> [indices]
        for idx, res in enumerate(dataset.resolutions):
            if res not in self.buckets:
                self.buckets[res] = []
            self.buckets[res].append(idx)
            
    def __iter__(self):
        batches = []
        for res, indices in self.buckets.items():
            if self.shuffle:
                # æ‰“ä¹±æ¡¶å†…ç´¢å¼•
                indices = torch.tensor(indices)[torch.randperm(len(indices))].tolist()
            
            # ç”Ÿæˆ batch
            for i in range(0, len(indices), self.batch_size):
                batch = indices[i:i + self.batch_size]
                if len(batch) == self.batch_size or not self.drop_last:
                    batches.append(batch)
        
        if self.shuffle:
            # æ‰“ä¹± batch é¡ºåº
            import random
            random.shuffle(batches)
            
        for batch in batches:
            yield batch

    def __len__(self):
        count = 0
        for indices in self.buckets.values():
            if self.drop_last:
                count += len(indices) // self.batch_size
            else:
                count += (len(indices) + self.batch_size - 1) // self.batch_size
        return count


def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """
    è‡ªå®šä¹‰ collate å‡½æ•°ã€‚æ”¯æŒä¸åŒåˆ†è¾¨ç‡çš„ latentï¼ˆè‡ªåŠ¨ paddingï¼‰ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ latents å…·æœ‰ç›¸åŒå½¢çŠ¶
    shapes = [item['latents'].shape for item in batch]
    all_same = all(s == shapes[0] for s in shapes)
    
    if all_same:
        # æ‰€æœ‰å½¢çŠ¶ç›¸åŒï¼Œç›´æ¥ stack
        latents = torch.stack([item['latents'] for item in batch])
    else:
        # å½¢çŠ¶ä¸åŒï¼Œéœ€è¦ padding åˆ°æœ€å¤§å°ºå¯¸
        max_h = max(s[1] for s in shapes)
        max_w = max(s[2] for s in shapes)
        
        # ç¡®ä¿å°ºå¯¸èƒ½è¢« patch_size=2 æ•´é™¤
        patch_size = 2
        max_h = ((max_h + patch_size - 1) // patch_size) * patch_size
        max_w = ((max_w + patch_size - 1) // patch_size) * patch_size
        
        padded_latents = []
        for item in batch:
            lat = item['latents']
            c, h, w = lat.shape
            if h < max_h or w < max_w:
                # Pad to max size (right and bottom padding)
                lat = torch.nn.functional.pad(
                    lat,
                    (0, max_w - w, 0, max_h - h),
                    mode='constant',
                    value=0
                )
            padded_latents.append(lat)
        
        latents = torch.stack(padded_latents)
        logger.debug(f"Padded latents from {shapes} to {latents.shape}")
    
    vl_embeds = [item['vl_embed'] for item in batch]  # ä¿æŒ list å½¢å¼
    
    return {
        'latents': latents,
        'vl_embed': vl_embeds,
    }


def create_dataloader(args) -> DataLoader:
    """
    ä»é…ç½®åˆ›å»º DataLoaderã€‚
    
    Args:
        args: è®­ç»ƒå‚æ•°ï¼ŒåŒ…å«dataset_configå’Œå…¶ä»–ç›¸å…³é…ç½®
        
    Returns:
        DataLoader: æ•°æ®åŠ è½½å™¨
    """
    # è¯»å– dataset é…ç½®
    if hasattr(args, 'dataset_config') and args.dataset_config:
        config = _read_dataset_config(args.dataset_config)
    else:
        config = {}
    
    # è·å–å‚æ•°
    datasets = config.get('datasets', [])
    
    # å…¼å®¹æ—§é…ç½® (å¦‚æœ config ä¸­æ²¡æœ‰ datasetsï¼Œå°è¯•ä» args æˆ–æ—§ config è¯»å–)
    if not datasets:
        cache_dir = config.get('cache_directory', getattr(args, 'cache_directory', None))
        if cache_dir:
            datasets = [{
                'cache_directory': cache_dir,
                'num_repeats': config.get('num_repeats', getattr(args, 'num_repeats', 1)),
                'resolution_limit': config.get('resolution_limit', None) # å…¼å®¹æ—§çš„ global limit
            }]
    
    if not datasets:
        raise ValueError("No datasets configured. Please check dataset_config.toml or arguments.")
    
    batch_size = config.get('batch_size', getattr(args, 'batch_size', 4))
    num_workers = config.get('num_workers', getattr(args, 'num_workers', 4))
    max_sequence_length = config.get('max_sequence_length', getattr(args, 'max_sequence_length', 512))
    
    # åˆ†æ¡¶è®¾ç½®ï¼š--disable_bucket ä¼˜å…ˆçº§æœ€é«˜
    if getattr(args, 'disable_bucket', False):
        enable_bucket = False
    else:
        enable_bucket = config.get('enable_bucket', getattr(args, 'enable_bucket', True))
    
    # åˆ›å»º dataset
    dataset = ZImageLatentDataset(
        datasets=datasets,
        max_sequence_length=max_sequence_length,
    )
    
    if enable_bucket:
        logger.info("ğŸŒŠ å¯ç”¨åˆ†æ¡¶ (BucketBatchSampler)")
        batch_sampler = BucketBatchSampler(
            dataset, 
            batch_size=batch_size,
            drop_last=True,
            shuffle=True
        )
        dataloader = DataLoader(
            dataset,
            batch_sampler=batch_sampler,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
        )
    else:
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
            drop_last=True,
        )
    
    logger.info("ğŸ“¦ DataLoader åˆ›å»ºå®Œæˆ")
    return dataloader


def _read_dataset_config(config_path: str) -> dict:
    """
    è¯»å– dataset é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    
    1. åˆå¹¶æ ¼å¼ (æ–°): [dataset] + [[dataset.sources]] åœ¨ä¸»é…ç½®ä¸­
    2. ç‹¬ç«‹æ ¼å¼ (æ—§): [general] + [[datasets]] åœ¨å•ç‹¬æ–‡ä»¶ä¸­
    3. æ—§æ ¼å¼: [dataset] å—
    """
    if toml is None:
        return {}
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = toml.load(f)
    
    # 1. åˆå¹¶æ ¼å¼ (æ–°): [dataset] + [[dataset.sources]] 
    #    ä¸»é…ç½®æ–‡ä»¶ä¸­çš„ dataset å—
    if 'dataset' in config:
        dataset_config = config['dataset'].copy()
        # å°† sources é‡å‘½åä¸º datasets (å…¼å®¹ create_dataloader)
        if 'sources' in dataset_config:
            dataset_config['datasets'] = dataset_config.pop('sources')
        return dataset_config
    
    # 2. ç‹¬ç«‹æ ¼å¼: [general] + [[datasets]]
    if 'datasets' in config:
        # å¦‚æœæœ‰ [general] å—ï¼Œåˆå¹¶åˆ°é¡¶å±‚
        if 'general' in config:
            config.update(config['general'])
        return config
    
    # 3. æ ¹çº§åˆ«é…ç½® (å…¼å®¹æ—§ç‰ˆ)
    return config