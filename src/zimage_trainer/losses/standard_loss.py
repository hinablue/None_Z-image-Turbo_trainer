# -*- coding: utf-8 -*-
"""
ğŸ¯ æ ‡å‡†è®­ç»ƒæŸå¤±å‡½æ•° (Standard Training Loss)

Charbonnier Loss + Cosine Loss ç»„åˆ

è¿™æ˜¯ Z-Image Trainer çš„æ ‡å‡†æŸå¤±å‡½æ•°ï¼Œç”¨äº Rectified Flow v-prediction è®­ç»ƒã€‚

æŸå¤±å…¬å¼ï¼š
L = Î»_l1 * Charbonnier(v_pred, v_target) + Î»_cos * (1 - cos_sim(v_pred, v_target))

å…¶ä¸­ï¼š
- Charbonnier: sqrt((v_pred - v_target)Â² + ÎµÂ²) - å¹³æ»‘ L1ï¼Œåœ¨ 0 ç‚¹å¯å¾®
- Cosine: 1 - cosine_similarity - é€Ÿåº¦æ–¹å‘ä¸€è‡´æ€§çº¦æŸ

è®¾è®¡ç†å¿µï¼š
- Charbonnier ä¿è¯åƒç´ çº§ç²¾åº¦ï¼Œå¯¹ç¦»ç¾¤å€¼é²æ£’
- Cosine ä¿è¯é€Ÿåº¦æ–¹å‘æ­£ç¡®ï¼Œå¯¹ Rectified Flow è‡³å…³é‡è¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Dict, Optional
import logging

logger = logging.getLogger(__name__)


class StandardLoss(nn.Module):
    """
    æ ‡å‡†è®­ç»ƒæŸå¤±ï¼šCharbonnier (Robust L1) + Cosine
    
    è¿™æ˜¯ Z-Image Trainer æ‰€æœ‰æ¨¡å‹çš„é»˜è®¤æŸå¤±å‡½æ•°ã€‚
    
    Args:
        lambda_l1: Charbonnier æŸå¤±æƒé‡ (é»˜è®¤ 1.0)
        lambda_cosine: Cosine æŸå¤±æƒé‡ (é»˜è®¤ 0.1)
        epsilon: Charbonnier å¹³æ»‘ç³»æ•° (é»˜è®¤ 1e-6)
    
    Example:
        >>> loss_fn = StandardLoss(lambda_l1=1.0, lambda_cosine=0.1)
        >>> loss, components = loss_fn(pred_v, target_v, return_components=True)
        >>> print(f"Total: {loss.item():.4f}, L1: {components['l1'].item():.4f}")
    """
    
    def __init__(
        self,
        lambda_l1: float = 1.0,
        lambda_cosine: float = 0.1,
        epsilon: float = 1e-6,
    ):
        super().__init__()
        self.lambda_l1 = lambda_l1
        self.lambda_cosine = lambda_cosine
        self.epsilon = epsilon
        
        logger.info(f"[StandardLoss] åˆå§‹åŒ–æ ‡å‡†æŸå¤±å‡½æ•°")
        logger.info(f"  Charbonnier(L1) æƒé‡: {lambda_l1}")
        logger.info(f"  Cosine æƒé‡: {lambda_cosine}")
        logger.info(f"  Epsilon: {epsilon}")
    
    def charbonnier(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Charbonnier Loss (å¹³æ»‘ L1)
        
        L = mean(sqrt((pred - target)Â² + ÎµÂ²))
        
        ä¼˜åŠ¿ï¼š
        - åœ¨ 0 ç‚¹å¯å¾®åˆ†ï¼ˆä¸ L1 ä¸åŒï¼‰
        - å¯¹ç¦»ç¾¤å€¼æ¯” L2 æ›´é²æ£’
        - ä¿æŒè¾¹ç¼˜é”åˆ©
        """
        diff = pred - target
        return torch.sqrt(diff ** 2 + self.epsilon ** 2).mean()
    
    def cosine_direction(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Cosine æ–¹å‘æŸå¤±
        
        L = 1 - cosine_similarity(pred, target)
        
        æ„ä¹‰ï¼š
        - çº¦æŸé¢„æµ‹é€Ÿåº¦ä¸ç›®æ ‡é€Ÿåº¦æ–¹å‘ä¸€è‡´
        - å¯¹ Rectified Flow éå¸¸é‡è¦ï¼ˆç›´çº¿è½¨è¿¹ï¼‰
        """
        pred_flat = pred.view(pred.shape[0], -1)
        target_flat = target.view(target.shape[0], -1)
        cos_sim = F.cosine_similarity(pred_flat, target_flat, dim=1).mean()
        return 1.0 - cos_sim
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        snr_weights: Optional[torch.Tensor] = None,
        return_components: bool = False,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—æ ‡å‡†æŸå¤±
        
        Args:
            pred_v: æ¨¡å‹é¢„æµ‹çš„é€Ÿåº¦ [B, C, H, W]
            target_v: ç›®æ ‡é€Ÿåº¦ [B, C, H, W]
            snr_weights: å¯é€‰çš„ Min-SNR æƒé‡ [B]
            return_components: æ˜¯å¦è¿”å›æŸå¤±åˆ†é‡
        
        Returns:
            loss: åŠ æƒåçš„æ€»æŸå¤±
            components: æŸå¤±åˆ†é‡å­—å…¸ {"l1": ..., "cosine": ...}
        """
        # è®¡ç®—å„åˆ†é‡
        loss_l1 = self.charbonnier(pred_v, target_v)
        loss_cosine = self.cosine_direction(pred_v, target_v)
        
        # åŠ æƒç»„åˆ
        loss = self.lambda_l1 * loss_l1 + self.lambda_cosine * loss_cosine
        
        # åº”ç”¨ SNR æƒé‡
        if snr_weights is not None:
            loss = loss * snr_weights.mean()
        
        components = {
            "l1": loss_l1,
            "cosine": loss_cosine,
        }
        
        if return_components:
            return loss, components
        return loss, components


def compute_standard_loss(
    pred_v: torch.Tensor,
    target_v: torch.Tensor,
    lambda_l1: float = 1.0,
    lambda_cosine: float = 0.1,
    epsilon: float = 1e-6,
    snr_weights: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè®¡ç®—æ ‡å‡†æŸå¤±
    
    Args:
        pred_v: é¢„æµ‹é€Ÿåº¦
        target_v: ç›®æ ‡é€Ÿåº¦
        lambda_l1: L1 æƒé‡
        lambda_cosine: Cosine æƒé‡
        epsilon: å¹³æ»‘ç³»æ•°
        snr_weights: SNR æƒé‡
    
    Returns:
        loss: æ€»æŸå¤±
        components: æŸå¤±åˆ†é‡ (å·²è½¬ä¸º float)
    """
    # Charbonnier
    diff = pred_v - target_v
    loss_l1 = torch.sqrt(diff ** 2 + epsilon ** 2).mean()
    
    # Cosine
    pred_flat = pred_v.view(pred_v.shape[0], -1)
    target_flat = target_v.view(target_v.shape[0], -1)
    cos_sim = F.cosine_similarity(pred_flat, target_flat, dim=1).mean()
    loss_cosine = 1.0 - cos_sim
    
    # ç»„åˆ
    loss = lambda_l1 * loss_l1 + lambda_cosine * loss_cosine
    
    # SNR åŠ æƒ
    if snr_weights is not None:
        loss = loss * snr_weights.mean()
    
    components = {
        "l1": loss_l1.item(),
        "cosine": loss_cosine.item(),
    }
    
    return loss, components
