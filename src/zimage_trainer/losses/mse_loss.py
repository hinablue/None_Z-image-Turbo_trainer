# -*- coding: utf-8 -*-
"""
ğŸ“ æ ‡å‡† MSE/L2 æŸå¤±å‡½æ•°æ¨¡å—

æä¾›æ ‡å‡†çš„å‡æ–¹è¯¯å·®æŸå¤±ï¼Œé€‚ç”¨äº Rectified Flow è®­ç»ƒã€‚

æŸå¤±ç±»å‹ï¼š
- MSELoss: æ ‡å‡† L2 æŸå¤± (Mean Squared Error)
- L2CosineLoss: L2 + Cosine æ–¹å‘æŸå¤±ç»„åˆ
- CharbonnierLoss: å¹³æ»‘ L1 æŸå¤± (Robust L1)

æ•°å­¦å…¬å¼ï¼š
- MSE: L = mean((v_pred - v_target)Â²)
- L2+Cosine: L = Î»_l2 * MSE + Î»_cos * (1 - cos_sim)
- Charbonnier: L = mean(sqrt((v_pred - v_target)Â² + ÎµÂ²))
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class MSELoss(nn.Module):
    """
    æ ‡å‡† L2/MSE æŸå¤±
    
    L = mean((v_pred - v_target)Â²)
    
    é€‚ç”¨äºï¼š
    - Rectified Flow v-prediction è®­ç»ƒ
    - éœ€è¦ä¸¥æ ¼åƒç´ çº§åŒ¹é…çš„åœºæ™¯
    """
    
    def __init__(
        self,
        reduction: str = "mean",  # "mean", "sum", "none"
    ):
        super().__init__()
        self.reduction = reduction
        logger.info(f"[MSELoss] åˆå§‹åŒ–æ ‡å‡† L2 æŸå¤± (reduction={reduction})")
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        return_components: bool = False,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®— MSE Loss
        
        Args:
            pred_v: æ¨¡å‹é¢„æµ‹çš„é€Ÿåº¦ [B, C, H, W]
            target_v: ç›®æ ‡é€Ÿåº¦ [B, C, H, W]
            return_components: æ˜¯å¦è¿”å›æŸå¤±åˆ†é‡
        
        Returns:
            loss: æ€»æŸå¤±
            components: æŸå¤±åˆ†é‡å­—å…¸
        """
        loss = F.mse_loss(pred_v, target_v, reduction=self.reduction)
        
        components = {"mse": loss}
        
        if return_components:
            return loss, components
        return loss, components


class CharbonnierLoss(nn.Module):
    """
    Charbonnier Loss (Robust L1 / å¹³æ»‘ L1)
    
    L = mean(sqrt((v_pred - v_target)Â² + ÎµÂ²))
    
    ä¼˜åŠ¿ï¼š
    - åœ¨ 0 ç‚¹å¯å¾®åˆ†ï¼Œè®­ç»ƒæ›´ç¨³å®š
    - å¯¹ç¦»ç¾¤å€¼æ¯” L2 æ›´é²æ£’
    - ä¿æŒè¾¹ç¼˜é”åˆ©åº¦ä¼˜äº L2
    """
    
    def __init__(
        self,
        epsilon: float = 1e-6,  # å¹³æ»‘ç³»æ•°
        reduction: str = "mean",
    ):
        super().__init__()
        self.epsilon = epsilon
        self.reduction = reduction
        logger.info(f"[CharbonnierLoss] åˆå§‹åŒ– Charbonnier æŸå¤± (Îµ={epsilon})")
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        return_components: bool = False,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®— Charbonnier Loss
        """
        diff = pred_v - target_v
        loss_per_element = torch.sqrt(diff ** 2 + self.epsilon ** 2)
        
        if self.reduction == "mean":
            loss = loss_per_element.mean()
        elif self.reduction == "sum":
            loss = loss_per_element.sum()
        else:  # none
            loss = loss_per_element
        
        components = {"charbonnier": loss}
        
        if return_components:
            return loss, components
        return loss, components


class L2CosineLoss(nn.Module):
    """
    L2 + Cosine ç»„åˆæŸå¤±
    
    L = Î»_l2 * MSE + Î»_cos * (1 - cosine_similarity)
    
    è®¾è®¡ç†å¿µï¼š
    - L2 (MSE): ä¿è¯åƒç´ çº§ç²¾åº¦
    - Cosine: ä¿è¯é€Ÿåº¦æ–¹å‘ä¸€è‡´æ€§
    
    é€‚ç”¨äº Rectified Flowï¼Œå…¶ä¸­é€Ÿåº¦æ–¹å‘æ¯”å¹…åº¦æ›´é‡è¦ã€‚
    """
    
    def __init__(
        self,
        lambda_l2: float = 1.0,      # L2 æŸå¤±æƒé‡
        lambda_cosine: float = 0.1,  # Cosine æŸå¤±æƒé‡
    ):
        super().__init__()
        self.lambda_l2 = lambda_l2
        self.lambda_cosine = lambda_cosine
        
        logger.info(f"[L2CosineLoss] åˆå§‹åŒ–ç»„åˆæŸå¤±")
        logger.info(f"  L2 æƒé‡: {lambda_l2}")
        logger.info(f"  Cosine æƒé‡: {lambda_cosine}")
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        return_components: bool = False,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®— L2 + Cosine Loss
        
        Args:
            pred_v: æ¨¡å‹é¢„æµ‹çš„é€Ÿåº¦ [B, C, H, W]
            target_v: ç›®æ ‡é€Ÿåº¦ [B, C, H, W]
            return_components: æ˜¯å¦è¿”å›æŸå¤±åˆ†é‡
        
        Returns:
            loss: æ€»æŸå¤±
            components: æŸå¤±åˆ†é‡å­—å…¸
        """
        # L2 æŸå¤±
        loss_l2 = F.mse_loss(pred_v, target_v)
        
        # Cosine æ–¹å‘æŸå¤±
        pred_flat = pred_v.view(pred_v.shape[0], -1)
        target_flat = target_v.view(target_v.shape[0], -1)
        cos_sim = F.cosine_similarity(pred_flat, target_flat, dim=1).mean()
        loss_cosine = 1.0 - cos_sim
        
        # ç»„åˆæŸå¤±
        loss = self.lambda_l2 * loss_l2 + self.lambda_cosine * loss_cosine
        
        components = {
            "mse": loss_l2,
            "cosine": loss_cosine,
        }
        
        if return_components:
            return loss, components
        return loss, components


class L1CosineLoss(nn.Module):
    """
    Charbonnier (L1) + Cosine ç»„åˆæŸå¤±
    
    L = Î»_l1 * Charbonnier + Î»_cos * (1 - cosine_similarity)
    
    è¿™æ˜¯ acrf_trainer ä½¿ç”¨çš„æ ‡å‡†æŸå¤±å‡½æ•°ã€‚
    """
    
    def __init__(
        self,
        lambda_l1: float = 1.0,      # Charbonnier æŸå¤±æƒé‡
        lambda_cosine: float = 0.1,  # Cosine æŸå¤±æƒé‡
        epsilon: float = 1e-6,        # Charbonnier å¹³æ»‘ç³»æ•°
    ):
        super().__init__()
        self.lambda_l1 = lambda_l1
        self.lambda_cosine = lambda_cosine
        self.epsilon = epsilon
        
        logger.info(f"[L1CosineLoss] åˆå§‹åŒ–ç»„åˆæŸå¤±")
        logger.info(f"  Charbonnier(L1) æƒé‡: {lambda_l1}")
        logger.info(f"  Cosine æƒé‡: {lambda_cosine}")
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        return_components: bool = False,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®— Charbonnier + Cosine Loss
        """
        # Charbonnier æŸå¤± (Robust L1)
        diff = pred_v - target_v
        loss_l1 = torch.sqrt(diff ** 2 + self.epsilon ** 2).mean()
        
        # Cosine æ–¹å‘æŸå¤±
        pred_flat = pred_v.view(pred_v.shape[0], -1)
        target_flat = target_v.view(target_v.shape[0], -1)
        cos_sim = F.cosine_similarity(pred_flat, target_flat, dim=1).mean()
        loss_cosine = 1.0 - cos_sim
        
        # ç»„åˆæŸå¤±
        loss = self.lambda_l1 * loss_l1 + self.lambda_cosine * loss_cosine
        
        components = {
            "charbonnier": loss_l1,
            "cosine": loss_cosine,
        }
        
        if return_components:
            return loss, components
        return loss, components


# === ä¾¿æ·å‡½æ•°æ¥å£ ===

def compute_mse_loss(
    pred_v: torch.Tensor,
    target_v: torch.Tensor,
) -> torch.Tensor:
    """è®¡ç®—æ ‡å‡† MSE Loss"""
    return F.mse_loss(pred_v, target_v)


def compute_charbonnier_loss(
    pred_v: torch.Tensor,
    target_v: torch.Tensor,
    epsilon: float = 1e-6,
) -> torch.Tensor:
    """è®¡ç®— Charbonnier Loss"""
    diff = pred_v - target_v
    return torch.sqrt(diff ** 2 + epsilon ** 2).mean()


def compute_cosine_loss(
    pred_v: torch.Tensor,
    target_v: torch.Tensor,
) -> torch.Tensor:
    """è®¡ç®— Cosine æ–¹å‘æŸå¤±"""
    pred_flat = pred_v.view(pred_v.shape[0], -1)
    target_flat = target_v.view(target_v.shape[0], -1)
    cos_sim = F.cosine_similarity(pred_flat, target_flat, dim=1).mean()
    return 1.0 - cos_sim
