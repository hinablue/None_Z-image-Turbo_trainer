# -*- coding: utf-8 -*-
"""
ğŸ¨ é¢‘åŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•° (Frequency-Aware Loss)

åŸºäºé¢‘åŸŸåˆ†ç¦»çš„è§£è€¦å­¦ä¹ ç­–ç•¥ï¼š
- é«˜é¢‘å¢å¼ºï¼šL1 Loss å¼ºåŒ–çº¹ç†/è¾¹ç¼˜ç»†èŠ‚
- ä½é¢‘é”å®šï¼šCosine Loss é”å®šç»“æ„/å…‰å½±æ–¹å‘

æ•°å­¦å…¬å¼ï¼š
L_total = L_base + Î»_hf * ||xÌ‚_high - x_high||â‚ + Î»_lf * (1 - cos(xÌ‚_low, x_low))

æ ¸å¿ƒä¼˜åŠ¿ï¼š
- è§£å†³å¾®è°ƒæ—¶"é¡¾æ­¤å¤±å½¼"é—®é¢˜ï¼ˆæå‡ç»†èŠ‚å´æåæ„å›¾ï¼‰
- é«˜é¢‘ç”¨ L1ï¼ˆä¿æŒè¾¹ç¼˜é”åˆ©ï¼‰ï¼Œä½é¢‘ç”¨ Cosineï¼ˆä¿ä½å…‰å½±ç»“æ„ï¼‰
- åœ¨ x0 ç©ºé—´åšé¢‘åŸŸåˆ†æï¼Œé¿å… v ç©ºé—´å«å™ªå¹²æ‰°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class FrequencyAwareLoss(nn.Module):
    """
    é¢‘åŸŸåˆ†ç¦»çš„æ··åˆæŸå¤±å‡½æ•°
    
    è®¾è®¡ç†å¿µï¼š
    1. ä» v-prediction åæ¨ xÌ‚â‚€ï¼ˆåœ¨å¹²å‡€ latent ç©ºé—´åšé¢‘åŸŸåˆ†æï¼‰
    2. ä½¿ç”¨é™é‡‡æ ·-ä¸Šé‡‡æ ·å¿«é€Ÿåˆ†ç¦»é«˜é¢‘/ä½é¢‘
    3. é«˜é¢‘ç”¨ L1 Lossï¼ˆè¾¹ç¼˜é”åˆ©ï¼‰ï¼Œä½é¢‘ç”¨ Cosine Lossï¼ˆæ–¹å‘ä¸€è‡´ï¼‰
    """
    
    def __init__(
        self,
        alpha_hf: float = 1.0,      # é«˜é¢‘å¢å¼ºæƒé‡
        beta_lf: float = 0.2,       # ä½é¢‘é”å®šæƒé‡
        base_weight: float = 1.0,   # åŸºç¡€ loss æƒé‡
        downsample_factor: int = 4, # ä½é¢‘æå–çš„é™é‡‡æ ·å› å­
        use_laplacian: bool = False, # æ˜¯å¦ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ï¼ˆæ›´ç²¾ç¡®ä½†æ›´æ…¢ï¼‰
        lf_magnitude_weight: float = 0.0,  # ä½é¢‘å¹…åº¦çº¦æŸï¼ˆé˜²æ­¢å‘ç°ï¼‰
    ):
        super().__init__()
        self.alpha_hf = alpha_hf
        self.beta_lf = beta_lf
        self.base_weight = base_weight
        self.downsample_factor = downsample_factor
        self.use_laplacian = use_laplacian
        self.lf_magnitude_weight = lf_magnitude_weight
        
        logger.info(f"[FreqLoss] åˆå§‹åŒ–é¢‘åŸŸæ„ŸçŸ¥æŸå¤±")
        logger.info(f"  é«˜é¢‘æƒé‡ (alpha_hf): {alpha_hf}")
        logger.info(f"  ä½é¢‘æƒé‡ (beta_lf): {beta_lf}")
        logger.info(f"  é™é‡‡æ ·å› å­: {downsample_factor}")
        
    def get_low_freq(self, x: torch.Tensor) -> torch.Tensor:
        """
        æå–ä½é¢‘åˆ†é‡ï¼ˆç»“æ„/å…‰å½±ï¼‰
        
        ä½¿ç”¨é™é‡‡æ ·-ä¸Šé‡‡æ ·æ–¹æ³•ï¼š
        - æ¯”é«˜æ–¯æ¨¡ç³Šæ›´å¿«
        - åœ¨ GPU ä¸Šæåº¦ä¼˜åŒ–
        - å¯¹ Latent (64x64 æˆ– 128x128) è¶³å¤Ÿè¿‡æ»¤çº¹ç†
        """
        h, w = x.shape[-2:]
        target_h = max(1, h // self.downsample_factor)
        target_w = max(1, w // self.downsample_factor)
        
        # é™é‡‡æ ·ï¼ˆæ»¤é™¤é«˜é¢‘ï¼‰
        x_small = F.adaptive_avg_pool2d(x, (target_h, target_w))
        
        # ä¸Šé‡‡æ ·è¿˜åŸå°ºå¯¸
        x_low = F.interpolate(x_small, size=(h, w), mode='bilinear', align_corners=False)
        
        return x_low
    
    def get_high_freq(self, x: torch.Tensor, x_low: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        æå–é«˜é¢‘åˆ†é‡ï¼ˆçº¹ç†/è¾¹ç¼˜ï¼‰
        
        é«˜é¢‘ = åŸå§‹ - ä½é¢‘
        """
        if x_low is None:
            x_low = self.get_low_freq(x)
        return x - x_low
    
    def reconstruct_x0_from_v(
        self,
        v_pred: torch.Tensor,
        noisy_latents: torch.Tensor,
        sigmas: torch.Tensor,
    ) -> torch.Tensor:
        """
        ä» v-prediction åæ¨ xÌ‚â‚€
        
        Z-Image å…¬å¼ï¼š
        x_t = (1 - Ïƒ) * x_0 + Ïƒ * noise
        v = noise - x_0
        
        åæ¨ï¼š
        x_0 = x_t - Ïƒ * v
        """
        # æ‰©å±• sigma ç»´åº¦ä»¥åŒ¹é… latents
        sigma_broadcast = sigmas.view(-1, 1, 1, 1)
        
        # åæ¨ x0
        pred_x0 = noisy_latents - sigma_broadcast * v_pred
        
        return pred_x0
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        noisy_latents: torch.Tensor,
        timesteps: torch.Tensor,
        num_train_timesteps: int = 1000,
        return_components: bool = False,
    ) -> torch.Tensor | Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—é¢‘åŸŸæ„ŸçŸ¥æŸå¤±
        
        Args:
            pred_v: æ¨¡å‹é¢„æµ‹çš„é€Ÿåº¦ v (B, C, H, W)
            target_v: ç›®æ ‡é€Ÿåº¦ v (B, C, H, W)
            noisy_latents: åŠ å™ªåçš„ latents x_t (B, C, H, W)
            timesteps: æ—¶é—´æ­¥ (B,)
            num_train_timesteps: æ€»è®­ç»ƒæ—¶é—´æ­¥æ•°
            return_components: æ˜¯å¦è¿”å›å„åˆ†é‡ loss
            
        Returns:
            total_loss: æ€»æŸå¤±
            components (optional): å„åˆ†é‡ loss å­—å…¸
        """
        # 1. åŸºç¡€ Lossï¼ˆä¿è¯æ¨¡å‹ä¸å´©ï¼‰
        base_loss = F.mse_loss(pred_v, target_v, reduction="mean")
        
        # 2. è®¡ç®— sigmaï¼ˆZ-Image: sigma = timestep / 1000ï¼‰
        # ä¿æŒè¾“å…¥å¼ é‡çš„ dtypeï¼Œé¿å…æ··åˆç²¾åº¦é—®é¢˜
        sigmas = timesteps.to(dtype=pred_v.dtype) / num_train_timesteps
        
        # 3. åæ¨ x0ï¼ˆåœ¨å¹²å‡€ latent ç©ºé—´åšé¢‘åŸŸåˆ†æï¼‰
        # æ³¨æ„ï¼šå¿…é¡»ä¿æŒæ¢¯åº¦æµï¼Œå› ä¸ºè¦ä¼˜åŒ– pred_v
        pred_x0 = self.reconstruct_x0_from_v(pred_v, noisy_latents, sigmas)
        target_x0 = self.reconstruct_x0_from_v(target_v, noisy_latents, sigmas)
        
        # 4. é¢‘åŸŸåˆ†ç¦»
        pred_low = self.get_low_freq(pred_x0)
        pred_high = pred_x0 - pred_low
        
        target_low = self.get_low_freq(target_x0)
        target_high = target_x0 - target_low
        
        # 5. é«˜é¢‘ Lossï¼šL1ï¼ˆä¿æŒè¾¹ç¼˜é”åˆ©ï¼‰
        # L1 å€¾å‘äºç¨€ç–è§£ï¼Œèƒ½äº§ç”Ÿæ›´é”åˆ©çš„è¾¹ç¼˜
        # MSE (L2) å€¾å‘äºå¹³å‡å€¼ï¼Œä¼šå¯¼è‡´æ¨¡ç³Š
        loss_hf = F.l1_loss(pred_high, target_high, reduction="mean")
        
        # 6. ä½é¢‘ Lossï¼šCosine Similarityï¼ˆé”å®šæ–¹å‘ï¼‰
        # åªé”æ–¹å‘ä¸é”æ¨¡é•¿ï¼Œå…è®¸ä¸€å®šçš„äº®åº¦è‡ªç”±åº¦
        pred_low_flat = pred_low.view(pred_low.shape[0], -1)
        target_low_flat = target_low.view(target_low.shape[0], -1)
        
        cos_sim = F.cosine_similarity(pred_low_flat, target_low_flat, dim=1)
        loss_lf_direction = (1 - cos_sim).mean()
        
        # å¯é€‰ï¼šä½é¢‘å¹…åº¦çº¦æŸï¼ˆé˜²æ­¢å‘ç°ï¼‰
        # ä¿æŒä¸ pred_v ç›¸åŒçš„ dtypeï¼Œé¿å…æ··åˆç²¾åº¦é—®é¢˜
        loss_lf_magnitude = torch.zeros(1, device=pred_v.device, dtype=pred_v.dtype).squeeze()
        if self.lf_magnitude_weight > 0:
            pred_norm = pred_low_flat.norm(dim=1)
            target_norm = target_low_flat.norm(dim=1)
            loss_lf_magnitude = F.mse_loss(pred_norm, target_norm)
        
        loss_lf = loss_lf_direction + self.lf_magnitude_weight * loss_lf_magnitude
        
        # 7. æ€» Loss
        total_loss = (
            self.base_weight * base_loss +
            self.alpha_hf * loss_hf +
            self.beta_lf * loss_lf
        )
        
        if return_components:
            components = {
                "base_loss": base_loss,
                "loss_hf": loss_hf,
                "loss_lf": loss_lf,
                "loss_lf_direction": loss_lf_direction,
                "loss_lf_magnitude": loss_lf_magnitude,
                "total_loss": total_loss,
            }
            return total_loss, components
        
        return total_loss


class AdaptiveFrequencyLoss(FrequencyAwareLoss):
    """
    è‡ªé€‚åº”é¢‘åŸŸæŸå¤±
    
    æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´é«˜é¢‘/ä½é¢‘æƒé‡ï¼š
    - è®­ç»ƒåˆæœŸï¼šä¾§é‡ä½é¢‘ï¼ˆå­¦ä¹ æ•´ä½“ç»“æ„ï¼‰
    - è®­ç»ƒåæœŸï¼šä¾§é‡é«˜é¢‘ï¼ˆç²¾ç‚¼ç»†èŠ‚ï¼‰
    """
    
    def __init__(
        self,
        alpha_hf_start: float = 0.1,
        alpha_hf_end: float = 1.0,
        beta_lf_start: float = 0.5,
        beta_lf_end: float = 0.1,
        warmup_steps: int = 100,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.alpha_hf_start = alpha_hf_start
        self.alpha_hf_end = alpha_hf_end
        self.beta_lf_start = beta_lf_start
        self.beta_lf_end = beta_lf_end
        self.warmup_steps = warmup_steps
        self.current_step = 0
        
    def update_step(self, step: int):
        """æ›´æ–°å½“å‰æ­¥æ•°ï¼Œè°ƒæ•´æƒé‡"""
        self.current_step = step
        
        if step < self.warmup_steps:
            # Warmup é˜¶æ®µï¼šä» start è¿‡æ¸¡åˆ° end
            progress = step / self.warmup_steps
            self.alpha_hf = self.alpha_hf_start + progress * (self.alpha_hf_end - self.alpha_hf_start)
            self.beta_lf = self.beta_lf_start + progress * (self.beta_lf_end - self.beta_lf_start)
        else:
            self.alpha_hf = self.alpha_hf_end
            self.beta_lf = self.beta_lf_end

