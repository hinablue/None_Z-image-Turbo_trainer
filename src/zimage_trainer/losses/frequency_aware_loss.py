# -*- coding: utf-8 -*-
"""
ğŸ¨ é¢‘åŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•° (Frequency-Aware Loss)

åŸºäºé¢‘åŸŸåˆ†ç¦»çš„è§£è€¦å­¦ä¹ ç­–ç•¥ï¼š
- é«˜é¢‘å¢å¼ºï¼šL1 Loss å¼ºåŒ–çº¹ç†/è¾¹ç¼˜ç»†èŠ‚
- ä½é¢‘é”å®šï¼šCosine Loss é”å®šç»“æ„/å…‰å½±æ–¹å‘

æ•°å­¦å…¬å¼ï¼š
L_total = L_base + Î»_hf * ||xÌ‚_high - x_high||â‚ + Î»_lf * (1 - cos(xÌ‚_low, x_low))

æ ¸å¿ƒä¼˜åŠ¿ï¼š
- è§£å†³å¾®è°ƒæ—¶"é¡¾æ­¤å¤±å½¼"é—®é¢˜ï¼ˆæå‡ç»†èŠ‚å´æåæ„å›¾ï¼‰
- é«˜é¢‘ç”¨ L1ï¼ˆä¿æŒè¾¹ç¼˜é”åˆ©ï¼‰ï¼Œä½é¢‘ç”¨ Cosineï¼ˆä¿ä½å…‰å½±ç»“æ„ï¼‰
- åœ¨ x0 ç©ºé—´åšé¢‘åŸŸåˆ†æï¼Œé¿å… v ç©ºé—´å«å™ªå¹²æ‰°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class FrequencyAwareLoss(nn.Module):
    """
    é¢‘åŸŸåˆ†ç¦»çš„æ··åˆæŸå¤±å‡½æ•°
    
    è®¾è®¡ç†å¿µï¼š
    1. ä» v-prediction åæ¨ xÌ‚â‚€ï¼ˆåœ¨å¹²å‡€ latent ç©ºé—´åšé¢‘åŸŸåˆ†æï¼‰
    2. ä½¿ç”¨é™é‡‡æ ·-ä¸Šé‡‡æ ·å¿«é€Ÿåˆ†ç¦»é«˜é¢‘/ä½é¢‘
    3. é«˜é¢‘ç”¨ L1 Lossï¼ˆè¾¹ç¼˜é”åˆ©ï¼‰ï¼Œä½é¢‘ç”¨ Cosine Lossï¼ˆæ–¹å‘ä¸€è‡´ï¼‰
    """
    
    def __init__(
        self,
        alpha_hf: float = 1.0,      # é«˜é¢‘å¢å¼ºæƒé‡
        beta_lf: float = 0.2,       # ä½é¢‘é”å®šæƒé‡
        downsample_factor: int = 4, # ä½é¢‘æå–çš„é™é‡‡æ ·å› å­
        # å‘åå…¼å®¹ï¼šæ¥å—ä½†å¿½ç•¥çš„æ—§å‚æ•°
        base_weight: float = 1.0,   # [å·²åºŸå¼ƒ] ç”±ä¸» L1 Loss è¦†ç›–
        lf_magnitude_weight: float = 0.0,  # [å·²åºŸå¼ƒ] ç”± StyleLoss.moments è¦†ç›–
        use_laplacian: bool = False, # [ä¿ç•™] æœªæ¥å¯ç”¨
    ):
        super().__init__()
        self.alpha_hf = alpha_hf
        self.beta_lf = beta_lf
        self.downsample_factor = downsample_factor
        
        logger.info(f"[FreqLoss] åˆå§‹åŒ–é¢‘åŸŸæ„ŸçŸ¥æŸå¤± (v2 åä½œæ¶æ„)")
        logger.info(f"  é«˜é¢‘æƒé‡ (alpha_hf): {alpha_hf}")
        logger.info(f"  ä½é¢‘æƒé‡ (beta_lf): {beta_lf}")
        logger.info(f"  é™é‡‡æ ·å› å­: {downsample_factor}")
        
    def get_low_freq(self, x: torch.Tensor) -> torch.Tensor:
        """
        æå–ä½é¢‘åˆ†é‡ï¼ˆç»“æ„/å…‰å½±ï¼‰
        
        ä½¿ç”¨é™é‡‡æ ·-ä¸Šé‡‡æ ·æ–¹æ³•ï¼š
        - æ¯”é«˜æ–¯æ¨¡ç³Šæ›´å¿«
        - åœ¨ GPU ä¸Šæåº¦ä¼˜åŒ–
        - å¯¹ Latent (64x64 æˆ– 128x128) è¶³å¤Ÿè¿‡æ»¤çº¹ç†
        """
        h, w = x.shape[-2:]
        target_h = max(1, h // self.downsample_factor)
        target_w = max(1, w // self.downsample_factor)
        
        # é™é‡‡æ ·ï¼ˆæ»¤é™¤é«˜é¢‘ï¼‰
        x_small = F.adaptive_avg_pool2d(x, (target_h, target_w))
        
        # ä¸Šé‡‡æ ·è¿˜åŸå°ºå¯¸
        x_low = F.interpolate(x_small, size=(h, w), mode='bilinear', align_corners=False)
        
        return x_low
    
    def get_high_freq(self, x: torch.Tensor, x_low: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        æå–é«˜é¢‘åˆ†é‡ï¼ˆçº¹ç†/è¾¹ç¼˜ï¼‰
        
        é«˜é¢‘ = åŸå§‹ - ä½é¢‘
        """
        if x_low is None:
            x_low = self.get_low_freq(x)
        return x - x_low
    
    def forward(
        self,
        pred_v: torch.Tensor,
        target_v: torch.Tensor,
        noisy_latents: torch.Tensor,
        timesteps: torch.Tensor,
        num_train_timesteps: int = 1000,
        return_components: bool = False,
    ) -> torch.Tensor | Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—é¢‘åŸŸæ„ŸçŸ¥æŸå¤± (v2 åä½œæ¶æ„)
        
        èŒè´£åˆ†å·¥ï¼š
        - é«˜é¢‘ (alpha_hf): L1 Loss å¢å¼ºçº¹ç†/è¾¹ç¼˜ç»†èŠ‚
        - ä½é¢‘ (beta_lf): Cosine Loss é”å®šç»“æ„/å…‰å½±æ–¹å‘
        
        æ³¨æ„ï¼šä¸å†åŒ…å« base_lossï¼Œç”±ä¸»è®­ç»ƒå¾ªç¯çš„ L1 Loss è´Ÿè´£
        
        Args:
            pred_v: æ¨¡å‹é¢„æµ‹çš„é€Ÿåº¦ v (B, C, H, W)
            target_v: ç›®æ ‡é€Ÿåº¦ v (B, C, H, W)
            noisy_latents: åŠ å™ªåçš„ latents x_t (B, C, H, W)
            timesteps: æ—¶é—´æ­¥ (B,)
            num_train_timesteps: æ€»è®­ç»ƒæ—¶é—´æ­¥æ•°
            return_components: æ˜¯å¦è¿”å›å„åˆ†é‡ loss
            
        Returns:
            total_loss: æ€»æŸå¤± (alpha_hf * loss_hf + beta_lf * loss_lf)
            components (optional): å„åˆ†é‡ loss å­—å…¸
        """
        # ä¿å­˜åŸå§‹ dtypeï¼Œç¡®ä¿æ‰€æœ‰è®¡ç®—ç»“æœéƒ½è½¬æ¢å›æ¥
        original_dtype = pred_v.dtype
        
        # è½¬æ¢ä¸º float32 è¿›è¡Œè®¡ç®—ï¼ˆé¿å…æ··åˆç²¾åº¦é—®é¢˜ï¼‰
        pred_v_fp32 = pred_v.float()
        target_v_fp32 = target_v.float()
        noisy_latents_fp32 = noisy_latents.float()
        
        # 1. è®¡ç®— sigmaï¼ˆZ-Image: sigma = timestep / 1000ï¼‰
        sigmas = timesteps.float() / num_train_timesteps
        
        # 2. åæ¨ x0ï¼ˆåœ¨å¹²å‡€ latent ç©ºé—´åšé¢‘åŸŸåˆ†æï¼‰
        sigma_broadcast = sigmas.view(-1, 1, 1, 1)
        pred_x0 = noisy_latents_fp32 - sigma_broadcast * pred_v_fp32
        target_x0 = noisy_latents_fp32 - sigma_broadcast * target_v_fp32
        
        # 3. é¢‘åŸŸåˆ†ç¦»
        pred_low = self.get_low_freq(pred_x0)
        pred_high = pred_x0 - pred_low
        
        target_low = self.get_low_freq(target_x0)
        target_high = target_x0 - target_low
        
        # 4. é«˜é¢‘ Lossï¼šL1ï¼ˆä¿æŒè¾¹ç¼˜é”åˆ©ï¼‰
        loss_hf = F.l1_loss(pred_high, target_high, reduction="mean")
        
        # 5. ä½é¢‘ Lossï¼šCosine Similarityï¼ˆé”å®šæ–¹å‘ï¼Œä¸çº¦æŸå¹…åº¦ï¼‰
        pred_low_flat = pred_low.view(pred_low.shape[0], -1)
        target_low_flat = target_low.view(target_low.shape[0], -1)
        
        cos_sim = F.cosine_similarity(pred_low_flat, target_low_flat, dim=1)
        loss_lf = (1.0 - cos_sim).mean()
        
        # 6. æ€» Lossï¼ˆçº¯é¢‘åŸŸåˆ†ç¦»ï¼Œæ— åŸºç¡€é¡¹ï¼‰
        total_loss = self.alpha_hf * loss_hf + self.beta_lf * loss_lf
        
        # è½¬æ¢å›åŸå§‹ dtype
        total_loss = total_loss.to(original_dtype)
        
        if return_components:
            components = {
                "loss_hf": loss_hf.to(original_dtype),
                "loss_lf": loss_lf.to(original_dtype),
                "total_loss": total_loss,
            }
            return total_loss, components
        
        return total_loss


class AdaptiveFrequencyLoss(FrequencyAwareLoss):
    """
    è‡ªé€‚åº”é¢‘åŸŸæŸå¤±
    
    æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´é«˜é¢‘/ä½é¢‘æƒé‡ï¼š
    - è®­ç»ƒåˆæœŸï¼šä¾§é‡ä½é¢‘ï¼ˆå­¦ä¹ æ•´ä½“ç»“æ„ï¼‰
    - è®­ç»ƒåæœŸï¼šä¾§é‡é«˜é¢‘ï¼ˆç²¾ç‚¼ç»†èŠ‚ï¼‰
    """
    
    def __init__(
        self,
        alpha_hf_start: float = 0.1,
        alpha_hf_end: float = 1.0,
        beta_lf_start: float = 0.5,
        beta_lf_end: float = 0.1,
        warmup_steps: int = 100,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.alpha_hf_start = alpha_hf_start
        self.alpha_hf_end = alpha_hf_end
        self.beta_lf_start = beta_lf_start
        self.beta_lf_end = beta_lf_end
        self.warmup_steps = warmup_steps
        self.current_step = 0
        
    def update_step(self, step: int):
        """æ›´æ–°å½“å‰æ­¥æ•°ï¼Œè°ƒæ•´æƒé‡"""
        self.current_step = step
        
        if step < self.warmup_steps:
            # Warmup é˜¶æ®µï¼šä» start è¿‡æ¸¡åˆ° end
            progress = step / self.warmup_steps
            self.alpha_hf = self.alpha_hf_start + progress * (self.alpha_hf_end - self.alpha_hf_start)
            self.beta_lf = self.beta_lf_start + progress * (self.beta_lf_end - self.beta_lf_start)
        else:
            self.alpha_hf = self.alpha_hf_end
            self.beta_lf = self.beta_lf_end

