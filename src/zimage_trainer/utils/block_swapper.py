# -*- coding: utf-8 -*-
"""
ğŸ”„ Forward Block Swapper - å‰å‘ä¼ æ’­å—äº¤æ¢å™¨

åœ¨æ¨¡å‹å‰å‘ä¼ æ’­æ—¶åŠ¨æ€ç®¡ç† Transformer å„å±‚åœ¨ GPU/CPU ä¹‹é—´çš„ç§»åŠ¨ï¼Œ
ä»¥èŠ‚çœ GPU æ˜¾å­˜ã€‚

åŸç†ï¼š
- åˆå§‹åŒ–æ—¶å°†å N å±‚ (blocks_to_swap) ç§»åˆ° CPU
- å‰å‘ä¼ æ’­æ—¶ï¼Œåœ¨æ¯å±‚è®¡ç®—å‰å°†å…¶ç§»åˆ° GPU
- è®¡ç®—å®Œæˆåï¼Œå¦‚æœè¯¥å±‚åœ¨äº¤æ¢èŒƒå›´å†…ï¼Œç§»å› CPU

æ³¨æ„ï¼š
- ä¼šå¢åŠ  CPUâ†”GPU æ•°æ®ä¼ è¾“å¼€é”€ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šä¸‹é™
- éœ€è¦ç¡®ä¿ CPU æœ‰è¶³å¤Ÿå†…å­˜å®¹çº³äº¤æ¢å‡ºçš„å±‚
- ä¸ gradient checkpointing å…¼å®¹
"""

import torch
import torch.nn as nn
from typing import List, Optional, Callable
import logging
import gc

logger = logging.getLogger(__name__)


class ForwardBlockSwapper:
    """
    å‰å‘ä¼ æ’­å—äº¤æ¢å™¨
    
    åœ¨æ¨¡å‹å‰å‘ä¼ æ’­æ—¶åŠ¨æ€ç®¡ç†å„å±‚åœ¨ GPU/CPU ä¹‹é—´çš„ç§»åŠ¨ã€‚
    """
    
    def __init__(
        self,
        blocks_to_swap: int,
        device: torch.device,
        verbose: bool = True,
    ):
        """
        Args:
            blocks_to_swap: è¦äº¤æ¢åˆ° CPU çš„å±‚æ•°
            device: GPU è®¾å¤‡
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        """
        self.blocks_to_swap = blocks_to_swap
        self.device = device
        self.cpu_device = torch.device("cpu")
        self.verbose = verbose
        
        # å±‚çŠ¶æ€è·Ÿè¸ª
        self.layers: Optional[nn.ModuleList] = None
        self.n_layers: int = 0
        self.swap_start_idx: int = 0  # ä»è¿™ä¸ªç´¢å¼•å¼€å§‹çš„å±‚éœ€è¦äº¤æ¢
        self.layer_on_gpu: List[bool] = []
        
        # ç»Ÿè®¡
        self.swap_in_count = 0
        self.swap_out_count = 0
        
    def setup(self, layers: nn.ModuleList):
        """
        è®¾ç½®è¦ç®¡ç†çš„å±‚å¹¶åˆå§‹åŒ–äº¤æ¢
        
        Args:
            layers: Transformer çš„å±‚åˆ—è¡¨ (nn.ModuleList)
        """
        self.layers = layers
        self.n_layers = len(layers)
        self.swap_start_idx = max(0, self.n_layers - self.blocks_to_swap)
        
        # åˆå§‹åŒ–çŠ¶æ€ï¼šæ‰€æœ‰å±‚å½“å‰éƒ½åœ¨ GPU
        self.layer_on_gpu = [True] * self.n_layers
        
        if self.blocks_to_swap <= 0:
            if self.verbose:
                logger.info("[BlockSwap] ç¦ç”¨ (blocks_to_swap=0)")
            return
        
        # å°†å N å±‚ç§»åˆ° CPU
        layers_moved = 0
        total_params = 0
        
        for i in range(self.swap_start_idx, self.n_layers):
            layer = self.layers[i]
            # è®¡ç®—å‚æ•°é‡
            layer_params = sum(p.numel() for p in layer.parameters())
            total_params += layer_params
            
            # ç§»åˆ° CPU
            layer.to(self.cpu_device)
            self.layer_on_gpu[i] = False
            layers_moved += 1
        
        # å¼ºåˆ¶æ¸…ç† GPU ç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        if self.verbose:
            param_mb = total_params * 2 / (1024 * 1024)  # å‡è®¾ BF16/FP16
            logger.info(f"[BlockSwap] å·²å°† {layers_moved} å±‚ç§»åˆ° CPU")
            logger.info(f"[BlockSwap] äº¤æ¢èŒƒå›´: layer[{self.swap_start_idx}] ~ layer[{self.n_layers-1}]")
            logger.info(f"[BlockSwap] é¢„è®¡èŠ‚çœæ˜¾å­˜: ~{param_mb:.1f} MB")
    
    def swap_in(self, layer_idx: int):
        """
        å°†æŒ‡å®šå±‚ç§»åˆ° GPU (å‰å‘ä¼ æ’­å‰è°ƒç”¨)
        
        Args:
            layer_idx: å±‚ç´¢å¼•
        """
        if self.blocks_to_swap <= 0:
            return
            
        if not self.layer_on_gpu[layer_idx]:
            self.layers[layer_idx].to(self.device)
            self.layer_on_gpu[layer_idx] = True
            self.swap_in_count += 1
    
    def swap_out(self, layer_idx: int):
        """
        å°†æŒ‡å®šå±‚ç§»å› CPU (å‰å‘ä¼ æ’­åè°ƒç”¨)
        
        Args:
            layer_idx: å±‚ç´¢å¼•
        """
        if self.blocks_to_swap <= 0:
            return
            
        # åªæœ‰åœ¨äº¤æ¢èŒƒå›´å†…çš„å±‚æ‰ç§»å› CPU
        if layer_idx >= self.swap_start_idx and self.layer_on_gpu[layer_idx]:
            self.layers[layer_idx].to(self.cpu_device)
            self.layer_on_gpu[layer_idx] = False
            self.swap_out_count += 1
    
    def get_stats(self) -> dict:
        """è·å–äº¤æ¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "blocks_to_swap": self.blocks_to_swap,
            "n_layers": self.n_layers,
            "swap_start_idx": self.swap_start_idx,
            "swap_in_count": self.swap_in_count,
            "swap_out_count": self.swap_out_count,
            "layers_on_gpu": sum(self.layer_on_gpu),
            "layers_on_cpu": self.n_layers - sum(self.layer_on_gpu),
        }
    
    def print_stats(self):
        """æ‰“å°äº¤æ¢ç»Ÿè®¡"""
        stats = self.get_stats()
        logger.info(f"[BlockSwap Stats] GPU: {stats['layers_on_gpu']} layers, CPU: {stats['layers_on_cpu']} layers")
        logger.info(f"[BlockSwap Stats] Swap operations: in={stats['swap_in_count']}, out={stats['swap_out_count']}")


def create_block_swapper(
    blocks_to_swap: int,
    device: torch.device,
    verbose: bool = True,
) -> Optional[ForwardBlockSwapper]:
    """
    åˆ›å»ºå—äº¤æ¢å™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        blocks_to_swap: è¦äº¤æ¢çš„å±‚æ•°ï¼Œ0 è¡¨ç¤ºç¦ç”¨
        device: GPU è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        ForwardBlockSwapper å®ä¾‹ï¼Œå¦‚æœ blocks_to_swap=0 è¿”å› None
    """
    if blocks_to_swap <= 0:
        return None
    
    return ForwardBlockSwapper(
        blocks_to_swap=blocks_to_swap,
        device=device,
        verbose=verbose,
    )
