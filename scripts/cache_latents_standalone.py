#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Z-Image Latent Cache Script (Standalone - Multi-GPU Safe)

ç‹¬ç«‹ç¼“å­˜è„šæœ¬ï¼Œé¿å…è§¦å‘ zimage_trainer/__init__.py å¯¼è‡´ CUDA æå‰åˆå§‹åŒ–ã€‚

Usage:
    python scripts/cache_latents_standalone.py \
        --vae /path/to/vae \
        --input_dir /path/to/images \
        --output_dir /path/to/cache \
        --resolution 1024
"""

import argparse
import logging
import os
from pathlib import Path
from typing import List, Tuple

from PIL import Image

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Z-Image architecture identifier
ARCHITECTURE = "zi"


def find_images(input_dir: str, extensions: Tuple[str, ...] = ('.jpg', '.jpeg', '.png', '.webp')) -> List[Path]:
    """æŸ¥æ‰¾ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡ (é€’å½’)"""
    input_path = Path(input_dir)
    images = set()
    for ext in extensions:
        images.update(input_path.rglob(f'*{ext}'))
        images.update(input_path.rglob(f'*{ext.upper()}'))
    return sorted(list(images))


def resize_image(image: Image.Image, resolution: int, bucket_no_upscale: bool = True) -> Image.Image:
    """è°ƒæ•´å›¾ç‰‡å¤§å°ï¼Œä¿æŒå®½é«˜æ¯”"""
    w, h = image.size
    
    # è®¡ç®—ç›®æ ‡å°ºå¯¸
    aspect = w / h
    if aspect > 1:
        # æ¨ªå‘å›¾ç‰‡
        new_w = resolution
        new_h = int(resolution / aspect)
    else:
        # çºµå‘å›¾ç‰‡
        new_h = resolution
        new_w = int(resolution * aspect)
    
    # å¯¹é½åˆ° 8 çš„å€æ•°ï¼ˆVAE è¦æ±‚ï¼‰
    new_w = (new_w // 8) * 8
    new_h = (new_h // 8) * 8
    
    # ä¸æ”¾å¤§
    if bucket_no_upscale:
        new_w = min(new_w, w)
        new_h = min(new_h, h)
        new_w = (new_w // 8) * 8
        new_h = (new_h // 8) * 8
    
    if (new_w, new_h) != (w, h):
        image = image.resize((new_w, new_h), Image.LANCZOS)
    
    return image


def process_image(
    image_path: Path,
    vae,
    resolution: int,
    output_dir: Path,
    device,  # torch.device
    dtype=None,  # torch.dtype
    input_root: Path = None,
) -> None:
    """å¤„ç†å•å¼ å›¾ç‰‡"""
    import torch
    from safetensors.torch import save_file
    
    if dtype is None:
        dtype = torch.bfloat16
    
    # åŠ è½½å›¾ç‰‡
    image = Image.open(image_path).convert('RGB')
    
    # è°ƒæ•´å¤§å°
    image = resize_image(image, resolution)
    w, h = image.size
    
    # è½¬æ¢ä¸º tensor
    import numpy as np
    img_array = np.array(image).astype(np.float32) / 255.0
    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
    
    # å½’ä¸€åŒ–åˆ° [-1, 1]
    img_tensor = img_tensor * 2.0 - 1.0
    img_tensor = img_tensor.to(device=device, dtype=dtype)
    
    # ç¼–ç 
    with torch.no_grad():
        latent = vae.encode(img_tensor).latent_dist.sample()
    
    # åº”ç”¨ scaling å’Œ shift (Pipeline format)
    scaling_factor = getattr(vae.config, 'scaling_factor', 0.3611)
    shift_factor = getattr(vae.config, 'shift_factor', 0.1159)
    latent = (latent - shift_factor) * scaling_factor
    
    # ä¿å­˜
    latent = latent.cpu()
    F, H, W = 1, latent.shape[2], latent.shape[3]
    dtype_str = "bf16" if dtype == torch.bfloat16 else "fp16"
    
    # è®¡ç®—è¾“å‡ºè·¯å¾„ (ä¿æŒç›®å½•ç»“æ„)
    if input_root:
        try:
            rel_path = image_path.relative_to(input_root)
            target_dir = output_dir / rel_path.parent
        except ValueError:
            target_dir = output_dir
    else:
        target_dir = output_dir
        
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶åæ ¼å¼: {name}_{WxH}_{arch}.safetensors
    name = image_path.stem
    output_file = target_dir / f"{name}_{w}x{h}_{ARCHITECTURE}.safetensors"
    
    # ä¿å­˜ä¸º safetensors
    sd = {f"latents_{F}x{H}x{W}_{dtype_str}": latent.squeeze(0)}
    save_file(sd, str(output_file))


def load_vae(vae_path: str, device, dtype):
    """åŠ è½½ VAEï¼ˆå»¶è¿Ÿå¯¼å…¥ torchï¼‰"""
    import torch
    from diffusers import AutoencoderKL
    
    logger.info(f"Loading VAE from {vae_path}")
    
    if os.path.isdir(vae_path):
        vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=dtype)
    elif vae_path.endswith(".safetensors"):
        vae = AutoencoderKL.from_single_file(vae_path, torch_dtype=dtype)
    else:
        raise ValueError(f"Unsupported VAE path format: {vae_path}")
    
    vae.to(device)
    vae.eval()
    vae.requires_grad_(False)
    
    logger.info(f"VAE loaded: scaling={getattr(vae.config, 'scaling_factor', 'N/A')}, "
                f"shift={getattr(vae.config, 'shift_factor', 'N/A')}")
    return vae


def worker_process(gpu_id: int, image_paths: List[Path], args, output_dir: Path, total_count: int, shared_counter, counter_lock):
    """å•ä¸ª GPU worker è¿›ç¨‹"""
    # å¿…é¡»åœ¨å¯¼å…¥ torch å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆé¿å… CUDA åˆå§‹åŒ–å†²çªï¼‰
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    # ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥ torch
    import torch
    
    device = torch.device("cuda:0")  # åœ¨è¿™ä¸ªè¿›ç¨‹ä¸­åªèƒ½çœ‹åˆ°ä¸€å¼ å¡
    dtype = torch.bfloat16
    
    # åŠ è½½ VAE
    print(f"[GPU {gpu_id}] Loading VAE...", flush=True)
    vae = load_vae(args.vae, device=device, dtype=dtype)
    print(f"[GPU {gpu_id}] VAE loaded, processing {len(image_paths)} images", flush=True)
    
    processed = 0
    for i, image_path in enumerate(image_paths):
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        name = image_path.stem
        existing = list(output_dir.glob(f"{name}_*_{ARCHITECTURE}.safetensors"))
        if args.skip_existing and existing:
            # è·³è¿‡ä½†ä»æ›´æ–°è®¡æ•°
            with counter_lock:
                shared_counter.value += 1
                current = shared_counter.value
            if current % 10 == 0 or current == total_count:
                print(f"Progress: {current}/{total_count}", flush=True)
            continue
        
        try:
            process_image(image_path, vae, args.resolution, output_dir, device, dtype, input_root=Path(args.input_dir))
            processed += 1
        except Exception as e:
            print(f"[GPU {gpu_id}] Error: {image_path.name}: {e}", flush=True)
        
        # æ›´æ–°å…±äº«è®¡æ•°å™¨å¹¶è¾“å‡ºè¿›åº¦
        with counter_lock:
            shared_counter.value += 1
            current = shared_counter.value
        
        # æ¯ 10 å¼ æˆ–æœ€åä¸€å¼ è¾“å‡ºè¿›åº¦
        if current % 10 == 0 or current == total_count:
            print(f"Progress: {current}/{total_count}", flush=True)
    
    # æ¸…ç†
    del vae
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    
    return processed


def main():
    parser = argparse.ArgumentParser(description="Cache latents for Z-Image training (Multi-GPU Safe)")
    parser.add_argument("--vae", type=str, required=True, help="VAE model path")
    parser.add_argument("--input_dir", type=str, required=True, help="Input image directory")
    parser.add_argument("--output_dir", type=str, required=True, help="Output cache directory")
    parser.add_argument("--resolution", type=int, default=1024, help="Target resolution")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size")
    parser.add_argument("--skip_existing", action="store_true", help="Skip existing cache files")
    parser.add_argument("--num_gpus", type=int, default=0, help="Number of GPUs (0=auto detect)")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾å›¾ç‰‡
    images = find_images(args.input_dir)
    total = len(images)
    print(f"Found {total} images", flush=True)
    
    if total == 0:
        print("No images to process", flush=True)
        return
    
    # æ£€æµ‹ GPU æ•°é‡ï¼ˆé¿å…åœ¨ä¸»è¿›ç¨‹åˆå§‹åŒ– CUDAï¼Œé˜²æ­¢ spawn æ¨¡å¼ä¸‹çš„å†²çªï¼‰
    if args.num_gpus > 0:
        num_gpus = args.num_gpus
    else:
        # ä½¿ç”¨ subprocess è°ƒç”¨ nvidia-smi è·å– GPU æ•°é‡ï¼Œé¿å…åˆå§‹åŒ– CUDA
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                num_gpus = len(result.stdout.strip().split('\n'))
            else:
                num_gpus = 1  # é»˜è®¤å•å¡
        except Exception:
            num_gpus = 1  # å¦‚æœ nvidia-smi ä¸å¯ç”¨ï¼Œé»˜è®¤å•å¡
    
    if num_gpus <= 1:
        # å• GPU æ¨¡å¼ï¼ˆå…¼å®¹åŸæœ‰é€»è¾‘ï¼‰
        # æ­¤æ—¶å¯ä»¥å®‰å…¨å¯¼å…¥ torch
        import torch
        
        print(f"Using single GPU mode", flush=True)
        print(f"Progress: 0/{total}", flush=True)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dtype = torch.bfloat16
        
        print(f"Loading VAE: {args.vae}", flush=True)
        vae = load_vae(args.vae, device=device, dtype=dtype)
        print("VAE loaded successfully", flush=True)
        
        processed = 0
        skipped = 0
        
        for i, image_path in enumerate(images, 1):
            name = image_path.stem
            existing = list(output_dir.glob(f"{name}_*_{ARCHITECTURE}.safetensors"))
            if args.skip_existing and existing:
                skipped += 1
                print(f"Progress: {i}/{total}", flush=True)
                continue
            
            try:
                process_image(image_path, vae, args.resolution, output_dir, device, dtype, input_root=Path(args.input_dir))
                processed += 1
                print(f"Progress: {i}/{total}", flush=True)
            except Exception as e:
                print(f"Error: {image_path}: {e}", flush=True)
                print(f"Progress: {i}/{total}", flush=True)
        
        print(f"Latent caching completed! Processed: {processed}, Skipped: {skipped}", flush=True)
        
        del vae
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("VAE unloaded, GPU memory released", flush=True)
    
    else:
        # å¤š GPU æ¨¡å¼
        import multiprocessing as mp
        from concurrent.futures import ProcessPoolExecutor, as_completed
        
        print(f"ğŸš€ Multi-GPU mode: using {num_gpus} GPUs", flush=True)
        print(f"Progress: 0/{total}", flush=True)
        
        # åˆ†ç‰‡
        chunk_size = (total + num_gpus - 1) // num_gpus
        chunks = []
        for i in range(num_gpus):
            start = i * chunk_size
            end = min(start + chunk_size, total)
            if start < total:
                chunks.append((i, images[start:end]))
        
        print(f"Distributing {total} images across {len(chunks)} GPUs", flush=True)
        for gpu_id, chunk in chunks:
            print(f"  GPU {gpu_id}: {len(chunk)} images", flush=True)
        
        # åˆ›å»ºå…±äº«è®¡æ•°å™¨å’Œé”
        manager = mp.Manager()
        shared_counter = manager.Value('i', 0)
        counter_lock = manager.Lock()
        
        # å¯åŠ¨è¿›ç¨‹æ± 
        mp.set_start_method('spawn', force=True)
        
        total_processed = 0
        with ProcessPoolExecutor(max_workers=num_gpus) as executor:
            futures = {
                executor.submit(worker_process, gpu_id, chunk, args, output_dir, total, shared_counter, counter_lock): gpu_id
                for gpu_id, chunk in chunks
            }
            
            for future in as_completed(futures):
                gpu_id = futures[future]
                try:
                    processed = future.result()
                    total_processed += processed
                    print(f"[GPU {gpu_id}] Completed: {processed} images", flush=True)
                except Exception as e:
                    print(f"[GPU {gpu_id}] Worker error: {e}", flush=True)
        
        print(f"Multi-GPU latent caching completed! Total processed: {total_processed}", flush=True)


if __name__ == "__main__":
    main()
